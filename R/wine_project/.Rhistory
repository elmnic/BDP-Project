authCorp <- tm_map(authCorp, toSpace, "”")
authCorp <- tm_map(authCorp, toSpace, "—")
authCorp <- tm_map(authCorp, removeNumbers)
authCorp <- tm_map(authCorp, removePunctuation)
authCorp <- tm_map(authCorp, removeWords, stopwords('english'), lazy = TRUE)
authCorp <- tm_map(authCorp, stemDocument, lazy = TRUE)
authCorp <- tm_map(authCorp, stripWhitespace, lazy = TRUE)
writeCorpus(authCorp)
authCorp <- VCorpus(DataframeSource(authFrame[3,]))
authCorp <- tm_map(authCorp,
content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
)
authCorp <- tm_map(authCorp, content_transformer(tolower))
authCorp <- tm_map(authCorp, toSpace, "-")
authCorp <- tm_map(authCorp, toSpace, "–")
authCorp <- tm_map(authCorp, toSpace, "%")
authCorp <- tm_map(authCorp, toSpace, "“")
authCorp <- tm_map(authCorp, toSpace, "”")
authCorp <- tm_map(authCorp, toSpace, "—")
authCorp <- tm_map(authCorp, removeNumbers)
authCorp <- tm_map(authCorp, removePunctuation)
authCorp <- tm_map(authCorp, removeWords, stopwords('english'), lazy = TRUE)
authCorp <- tm_map(authCorp, stemDocument, lazy = TRUE)
authCorp <- tm_map(authCorp, stripWhitespace, lazy = TRUE)
writeCorpus(authCorp)
for (row in 4:nrow(authFrame)) {
authCorp <- VCorpus(DataframeSource(authFrame[row,]))
authCorp <- tm_map(authCorp,
content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
)
authCorp <- tm_map(authCorp, content_transformer(tolower))
authCorp <- tm_map(authCorp, toSpace, "-")
authCorp <- tm_map(authCorp, toSpace, "–")
authCorp <- tm_map(authCorp, toSpace, "%")
authCorp <- tm_map(authCorp, toSpace, "“")
authCorp <- tm_map(authCorp, toSpace, "”")
authCorp <- tm_map(authCorp, toSpace, "—")
authCorp <- tm_map(authCorp, removeNumbers)
authCorp <- tm_map(authCorp, removePunctuation)
authCorp <- tm_map(authCorp, removeWords, stopwords('english'), lazy = TRUE)
authCorp <- tm_map(authCorp, stemDocument, lazy = TRUE)
authCorp <- tm_map(authCorp, stripWhitespace, lazy = TRUE)
writeCorpus(authCorp)
}
library(tm)
library(SnowballC)
library(dplyr)
library(tidyverse)
library(ngram)
library(stylo)
toSpace <- content_transformer(
function(x, pattern) {
return (gsub(pattern, " ", x))
})
file <- "./winemag-data-130k-v2.csv"
csv <- read.csv(file, header = TRUE)
# keepCols <- c("doc_id", "text", "points", "taster_name")
# csv2 <- csv[keepCols]
authFrame <- aggregate(csv['text'], csv['taster_name'], paste, collapse = ' ')
names(authFrame) == "taster_name" <- "doc_id"
[names(authFrame) == "taster_name"] <- "doc_id"
dtm <- DocumentTermMatrix(authCorp)
dtm
dtm
inspect(dtm[1:2, 1:5])
wine-corpus <- Corpus(DirSource("./complete-docs"))
wine-corpus <- Corpus(DirSource("./complete-docs/"))
getwd()
wine-corpus <- Corpus(DirSource(list("./complete-docs/")))
wine-corpus <- Corpus(DirSource("./complete-docs/"))
doc-source <- "./complete-docs"
library(tm)
library(SnowballC)
library(dplyr)
library(tidyverse)
library(ngram)
library(stylo)
toSpace <- content_transformer(
function(x, pattern) {
return (gsub(pattern, " ", x))
})
file <- "./winemag-data-130k-v2.csv"
csv <- read.csv(file, header = TRUE)
authFrame <- aggregate(csv['text'], csv['taster_name'], paste, collapse = ' ')
colnames(authFrame)[names(authFrame) == "taster_name"] <- "doc_id"
authFrame <- authFrame[-c(1),]
doc-source <- "./complete-docs"
doc_source <- "./complete-docs"
wine_corpus <- Corpus(DirSource(doc-source))
wine_corpus <- Corpus(DirSource(doc_source))
?Corpus
getwd()
doc_source <- "./complete-docs"
wine_corpus <- Corpus(DirSource(doc_source))
wine_corpus <- Corpus(DirSource(doc_source, encoding = "UTF-8-MAC"))
getwd()
doc_source <- getwd() + "./complete-docs"
doc_source <- getwd() + "/complete-docs"
library(tm)
library(SnowballC)
library(dplyr)
library(tidyverse)
library(ngram)
library(stylo)
toSpace <- content_transformer(
function(x, pattern) {
return (gsub(pattern, " ", x))
})
file <- "./winemag-data-130k-v2.csv"
csv <- read.csv(file, header = TRUE)
authFrame <- aggregate(csv['text'], csv['taster_name'], paste, collapse = ' ')
colnames(authFrame)[names(authFrame) == "taster_name"] <- "doc_id"
authFrame <- authFrame[-c(1),]
doc_source <- "./complete-docs"
wine_corpus <- Corpus(DirSource(doc_source))
wine_corpus <- Corpus(DirSource(doc_source, pattern = ".txt"))
getwd()
cd()
doc_source <- "./complete-docs/"
wine_corpus <- Corpus(DirSource(doc_source, pattern = ".txt"))
library(tm)
library(SnowballC)
library(dplyr)
library(tidyverse)
library(ngram)
library(wordcloud)
toSpace <- content_transformer(
function(x, pattern) {
return (gsub(pattern, " ", x))
})
doc_source <- "./complete_docs/"
wine_corpus <- Corpus(DirSource(directory = doc_source , encoding = "UTF-8"))
dtm <- DocumentTermMatrix(wine_corpus)
dtm
inspect(dtm[1:2, 1:5])
dataset <- as.data.frame(as.matrix(dtm))
dataset
inspect(dataset[1:2, 1:5])
dataset[head(dataset)]
dataset <- as.data.frame(as.matrix(dtm))
dtm <- DocumentTermMatrix(wine_corpus, control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
dtm
inspect(dtm[1:2, 1:5])
dtm <- DocumentTermMatrix(wine_corpus, control = list(weighting = function(x) weightTfIdf(x, normalize = TRUE)))
dtm
inspect(dtm[1:2, 1:5])
freq <- colSums(as.matrix(dtm))
dataset <- as.data.frame(as.matrix(dtm))
length(freq)
ord <- order(freq,decreasing=TRUE)
freq[head(ord)]
freq[tail(ord)]
dtm <- DocumentTermMatrix(wine_corpus, control = list(weighting = function(x) weightTfIdf(x, normalize = TRUE)))
dtm
inspect(dtm[1:2, 1:5])
freq <- colSums(as.matrix(dtm))
dataset <- as.data.frame(as.matrix(dtm))
length(freq)
ord <- order(freq,decreasing=TRUE)
freq[head(ord)]
freq[tail(ord)]
freq[tail(ord)]
dataML
library(tm)
library(SnowballC)
library(dplyr)
library(tidyverse)
library(ngram)
library(wordcloud)
toSpace <- function(x, pattern) {
return (gsub(pattern, " ", x))
}
toBlank <- function(x, pattern) {
return (gsub(pattern, "", x))
}
# Needed for preprocessing dataset #
file <- "./winemag-data-130k-v2.csv"
csv <- read.csv(file, header = TRUE)
#authFrame <- aggregate(csv['text'], csv['taster_name'], paste, collapse = ' ')
colnames(authFrame)[names(authFrame) == "taster_name"] <- "doc_id"
# Needed for machine learning #
keepCols <- c("doc_id", "text", "points", "taster_name")
dataML <- csv[keepCols]
library(tm)
library(SnowballC)
library(dplyr)
library(tidyverse)
library(ngram)
library(wordcloud)
toSpace <- function(x, pattern) {
return (gsub(pattern, " ", x))
}
toBlank <- function(x, pattern) {
return (gsub(pattern, "", x))
}
# Needed for preprocessing dataset #
file <- "./winemag-data-130k-v2.csv"
csv <- read.csv(file, header = TRUE)
# Needed for machine learning #
keepCols <- c("doc_id", "text", "points", "taster_name")
dataML <- csv[keepCols]
dataML
dataML <- dataML[-c(1)]
dataML
# Needed for machine learning #
keepCols <- c("doc_id", "text", "points", "taster_name")
dataML <- csv[keepCols]
dataML <- dataML[!(dataML$taster_name == ""),]
dataML
i <- sapply(dataML, is.factor)
dataML[i] <- lapply(dataML[i], as.character)
dataML$text <- lapply(dataML$text, tolower)
dataML$text <- lapply(dataML$text, toSpace, "-")
dataML$text <- lapply(dataML$text, toSpace, "–")
dataML$text <- lapply(dataML$text, toSpace, "%")
dataML$text <- lapply(dataML$text, toSpace, "“")
dataML$text <- lapply(dataML$text, toSpace, "”")
dataML$text <- lapply(dataML$text, toSpace, "\\d")
dataML$text <- lapply(dataML$text, toBlank, "[[:punct:]]")
dataML$text <- lapply(dataML$text, removeWords, stopwords("english"))
dataML$text <- lapply(dataML$text, toBlank, "^\\s+")
dataML$text <- lapply(dataML$text, stemDocument)
dataML$text <- as.character(dataML$text)
write.csv(dataML, "./ml_docs/wine_ml.csv", row.names=F)
library(tm)
library(SnowballC)
library(dplyr)
library(tidyverse)
library(ngram)
library(wordcloud)
toSpace <- function(x, pattern) {
return (gsub(pattern, " ", x))
}
toBlank <- function(x, pattern) {
return (gsub(pattern, "", x))
}
# Needed for preprocessing dataset #
file <- "./winemag-data-130k-v2.csv"
library(tm)
library(SnowballC)
library(dplyr)
library(tidyverse)
library(ngram)
library(wordcloud)
toSpace <- function(x, pattern) {
return (gsub(pattern, " ", x))
}
toBlank <- function(x, pattern) {
return (gsub(pattern, "", x))
}
# Needed for preprocessing dataset #
file <- "./winemag-data-130k-v2.csv"
csv <- read.csv(file, header = TRUE)
# Needed for machine learning #
keepCols <- c("doc_id", "text", "points", "taster_name")
dataML <- csv[keepCols]
dataML <- dataML[!(dataML$taster_name == ""),]
View(dataML)
dataML <- dataML[!(dataML$taster_name == ""),]
dataML
i <- sapply(dataML, is.factor)
dataML[i] <- lapply(dataML[i], as.character)
dataML$text <- lapply(dataML$text, tolower)
dataML$text <- lapply(dataML$text, toSpace, "-")
dataML$text <- lapply(dataML$text, toSpace, "–")
dataML$text <- lapply(dataML$text, toSpace, "%")
dataML$text <- lapply(dataML$text, toSpace, "“")
dataML$text <- lapply(dataML$text, toSpace, "”")
dataML$text <- lapply(dataML$text, toSpace, "\\d")
dataML$text <- lapply(dataML$text, toBlank, "[[:punct:]]")
dataML$text <- lapply(dataML$text, removeWords, stopwords("english"))
dataML$text <- lapply(dataML$text, toBlank, "^\\s+")
dataML$text <- lapply(dataML$text, stemDocument)
dataML$text <- as.character(dataML$text)
# Get top 500~ words used
words = dataML$text.unique()
# Get top 500~ words used
words = unique(dataML$text)
# Get top 500~ words used
word_corpus = Corpus(DataframeSource(dataML$text))
# Get top 500~ words used
word_corpus = Corpus(DataframeSource(dataML['doc_id', 'text']))
# Get top 500~ words used
word_corpus = Corpus(DataframeSource(dataML))
dtm <- DocumentTermMatrix(word_corpus)
View(dtm)
freq <- findFreqTerms(dtm)
freq <- findMostFreqTerms(dtm, n = 500)
View(freq)
freq[["0"]]
# Get top 500~ words used
word_corpus = Corpus(VectorSource(dataML$text))
dtm <- DocumentTermMatrix(word_corpus)
freq <- findMostFreqTerms(dtm, n = 500)
install.packages("qdap")
library(qdap)
library("qdap", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
install.packages("rJava")
# Get top 500~ words used
word_corpus = Corpus(VectorSource(dataML$text))
dtm <- DocumentTermMatrix(word_corpus)
freq <- findMostFreqTerms(dtm, n = 500)
View(freq)
freq <- findMostFreqTerms(dtm, INDEX = rep(1 : nrow(dtm), each = 500L))
?findFreqTerms
# Get top 500~ words used
word_corpus = Corpus(VectorSource(dataML$text))
dtm <- DocumentTermMatrix(word_corpus)
freq <- findFreqTerms(dtm)
m <- as.matrix(freq)
top <- sort(rowSums(m), decreasing = T)
dtm <- DocumentTermMatrix(word_corpus)
m <- as.matrix(dtm)
m <- as.matrix(dtm)
View(word_corpus)
library(tm)
library(SnowballC)
library(dplyr)
library(tidyverse)
library(slam)
library(stringr)
library(mltools)
library(data.table)
toSpace <- function(x, pattern) {
return (gsub(pattern, " ", x))
}
toBlank <- function(x, pattern) {
return (gsub(pattern, "", x))
}
# Needed for preprocessing dataset #
file <- "./winemag-data-130k-v2.csv"
csv <- read.csv(file, header = TRUE)
# Needed for machine learning #
keepCols <- c("doc_id", "text", "points", "taster_name")
dataML <- csv[keepCols]
# Remove empty taster
dataML <- dataML[!(dataML$taster_name == ""),]
# dataML
# Clean and process text
i <- sapply(dataML, is.factor)
dataML[i] <- lapply(dataML[i], as.character)
dataML$text <- lapply(dataML$text, tolower)
dataML$text <- lapply(dataML$text, toSpace, "-")
dataML$text <- lapply(dataML$text, toSpace, "–")
dataML$text <- lapply(dataML$text, toSpace, "%")
dataML$text <- lapply(dataML$text, toSpace, "“")
dataML$text <- lapply(dataML$text, toSpace, "”")
dataML$text <- lapply(dataML$text, toSpace, "\\d")
dataML$text <- lapply(dataML$text, toBlank, "[[:punct:]]")
dataML$text <- lapply(dataML$text, removeWords, stopwords("english"))
dataML$text <- lapply(dataML$text, toBlank, "^\\s+")
dataML$text <- lapply(dataML$text, stemDocument)
dataML$text <- as.character(dataML$text)
# Save for later
textData <- dataML$text
dataML$text <- textData
# Get top 500 words used
word_corpus <- Corpus(VectorSource(dataML$text))
dtm <- DocumentTermMatrix(word_corpus)
inspect(dtm[1:4, 1:5])
# Sum all terms
agg_dtm <- col_sums(dtm) %>%
sort(decreasing = T)
agg_dtm <- agg_dtm[1:100]
keepWords <- names(agg_dtm)
head(agg_dtm, 5)
# Filter the most common terms from the data frame
dataML$text <- lapply(dataML$text, FUN = function(x) {
lapply(strsplit(x, "\\s"), FUN = function(y) {
paste(intersect(y, keepWords), collapse=" ")
})
})
# Flatten text into one string
flattenedText <- unlist(dataML$text) %>%
strsplit(split=" ") %>%
unlist()
write.csv(keepWords, "./ml_docs/keepWords.csv", row.names=F, col.names=F)
library(tm)
library(SnowballC)
library(dplyr)
library(tidyverse)
library(slam)
library(stringr)
library(mltools)
library(data.table)
toSpace <- function(x, pattern) {
return (gsub(pattern, " ", x))
}
toBlank <- function(x, pattern) {
return (gsub(pattern, "", x))
}
# Needed for preprocessing dataset #
file <- "./winemag-data-130k-v2.csv"
csv <- read.csv(file, header = TRUE)
# Needed for machine learning #
keepCols <- c("doc_id", "text", "points", "taster_name")
dataML <- csv[keepCols]
# Remove empty taster
dataML <- dataML[!(dataML$taster_name == ""),]
# dataML
# Clean and process text
i <- sapply(dataML, is.factor)
dataML[i] <- lapply(dataML[i], as.character)
dataML$text <- lapply(dataML$text, tolower)
dataML$text <- lapply(dataML$text, toSpace, "-")
dataML$text <- lapply(dataML$text, toSpace, "–")
dataML$text <- lapply(dataML$text, toSpace, "%")
dataML$text <- lapply(dataML$text, toSpace, "“")
dataML$text <- lapply(dataML$text, toSpace, "”")
dataML$text <- lapply(dataML$text, toSpace, "\\d")
dataML$text <- lapply(dataML$text, toBlank, "[[:punct:]]")
dataML$text <- lapply(dataML$text, removeWords, stopwords("english"))
dataML$text <- lapply(dataML$text, toBlank, "^\\s+")
dataML$text <- lapply(dataML$text, stemDocument)
dataML$text <- as.character(dataML$text)
# Save for later
textData <- dataML$text
dataML$text <- textData
# Get top 500 words used
word_corpus <- Corpus(VectorSource(dataML$text))
dtm <- DocumentTermMatrix(word_corpus)
inspect(dtm[1:4, 1:5])
# Sum all terms
agg_dtm <- col_sums(dtm) %>%
sort(decreasing = T)
agg_dtm <- agg_dtm[1:100]
keepWords <- names(agg_dtm)
head(agg_dtm, 5)
# Filter the most common terms from the data frame
dataML$text <- lapply(dataML$text, FUN = function(x) {
lapply(strsplit(x, "\\s"), FUN = function(y) {
paste(intersect(y, keepWords), collapse=" ")
})
})
dataML$text[is.na(dataML$text)] <- ""
dataML$text <- unlist(dataML$text)
dataML$text[is.na(dataML$text)] <- ""
write.csv(dataML, "wine_ml.csv", row.names=F)
dataML$text[is.na(dataML$text)] <- " "
write.csv(dataML, "wine_ml.csv", row.names=F)
dataML$text[is.na(dataML$text)]
dataML$text[is.na(dataML$text)] <- ""
dataML$text[is.na(dataML$text)]
View(dataML)
# dataML$text[is.na(dataML$text)] <- ""
dataML <- dataML[complete.cases(dataML$text)]
install.packages("janitor")
dataML$text <- unlist(dataML$text)
# dataML$text[is.na(dataML$text)] <- ""
dataML <- dataML[dataML$text != ""]
# dataML$text[is.na(dataML$text)] <- ""
dataML <- dataML[dataML$text != "",]
View(dataML)
write.csv(dataML, "wine_ml.csv", row.names=F)
dataML <- dataML[dataML$text != " ",]
View(dataML)
write.csv(dataML, "wine_ml.csv", row.names=F)
keepWords[:10]
keepWords[1:10]
keepWords
inspect(dtm[1:4, 1:5])
dtm[1:4, 1:5]
# Create DT-matrix
word_corpus <- Corpus(VectorSource(dataML$text))
library(tm)
library(SnowballC)
library(dplyr)
library(tidyverse)
library(slam)
library(stringr)
library(mltools)
library(data.table)
# Create DT-matrix
word_corpus <- Corpus(VectorSource(dataML$text))
dtm <- DocumentTermMatrix(word_corpus)
dtm[1:4, 1:5]
inspect(dtm[1:4, 1:5])
agg_dtm[1:10]
library(tm)
library(SnowballC)
library(dplyr)
library(tidyverse)
library(slam)
library(stringr)
library(mltools)
library(data.table)
toSpace <- function(x, pattern) {
return (gsub(pattern, " ", x))
}
toBlank <- function(x, pattern) {
return (gsub(pattern, "", x))
}
# Needed for preprocessing dataset #
file <- "./winemag-data-130k-v2.csv"
csv <- read.csv(file, header = TRUE)
# Needed for machine learning #
keepCols <- c("doc_id", "text", "points", "taster_name")
dataML <- csv[keepCols]
# Remove empty taster
dataML <- dataML[!(dataML$taster_name == ""),]
# Clean and process text
i <- sapply(dataML, is.factor)
dataML[i] <- lapply(dataML[i], as.character)
dataML$text <- lapply(dataML$text, tolower)
dataML$text <- lapply(dataML$text, toSpace, "-")
dataML$text <- lapply(dataML$text, toSpace, "–")
dataML$text <- lapply(dataML$text, toSpace, "%")
dataML$text <- lapply(dataML$text, toSpace, "“")
dataML$text <- lapply(dataML$text, toSpace, "”")
dataML$text <- lapply(dataML$text, toSpace, "\\d")
dataML$text <- lapply(dataML$text, toBlank, "[[:punct:]]")
dataML$text <- lapply(dataML$text, removeWords, stopwords("english"))
